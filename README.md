## Running machine learning models locally

This is a collection of links and information to assess the viability of
running machine learning models and large language models locally. The aim is
to support engineers and stakeholders to make a well-informed decision when
procuring LLM infrastructure.

Analysis and links are at [local_ml_models.md](local_ml_models.md).

Actual hardware configuration is detailed in
[local_ml_hardware_alternatives.md](local_ml_hardware_alternatives.md#server).
