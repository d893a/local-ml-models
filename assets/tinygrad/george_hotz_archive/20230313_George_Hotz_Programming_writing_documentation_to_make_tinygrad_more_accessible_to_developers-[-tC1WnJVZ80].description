Date of stream 13 Mar 2023.
from $1499 buy https://comma.ai/shop/comma-three
Live-stream chat added as Subtitles/CC - English (Twitch Chat) - three-dot menu icon - Show transcript

Source files:
- https://github.com/geohot/tinygrad/tree/master/docs
- https://github.com/geohot/tinygrad
Follow for notifications:
- https://twitch.tv/georgehotz
Support George:
- https://twitch.tv/subs/georgehotz
Programming playlist:
- https://www.youtube.com/playlist?list=PLzFUMGbVxlQs5s-LNAyKgcq5SL28ZLLKC

Chapters:
00:00:00 intro
00:00:45 tinygrad docs
00:01:10 triton rst docs
00:02:10 debt in tinygrad
00:03:05 great stream to learn how everything works
00:05:25 george teaching you programming
00:06:25 2+3 as Tensor
00:10:25 Types from Tensor
00:11:00 accountants problems, integrity
00:11:40 boilerplate imports
00:12:18 vscode python annotations %%
00:16:10 understanding the example 2+3 as a Tensor
00:17:50 derivatives in tinygrad
00:21:20 Tensor, relu, mlops example
00:24:45 LazyBuffer
00:25:20 pip or install it via git clone
00:26:10 LazyBuffer
00:29:55 lazydata
00:33:35 lazyop
00:41:10 hlop, mlop
00:41:25 thinking about this for 2.5 years
00:41:40 tinygrad hope
00:41:55 worth reading to understand hlop, mlop
00:42:25 tinygrad not slow, pytorch and tensorflow hundred of ops 
00:43:15 getting pytorch support hard, why are things slow
00:44:00 worth reading
00:46:45 why can't FROMCPU be folded into ASTs? 
00:48:45 example is tinygrad fast 
00:49:30 DeviceBuffer
00:52:00 subclass c++
00:53:30 InterpreteredBuffer
00:54:35 don't worry about lazybuffer
00:55:10 DeviceBuffer
00:57:35 inops support
00:58:20 lazy.py
01:01:10 RawBuffer, 2+3 in raw clang
01:05:45 2+3 autogenerated clang code
01:10:55 ShapeTracker
01:23:55 no Tensor just a shape
01:24:25 variable in shape/symolic.py
01:33:00 theory of lines of code
01:33:50 new issues on github
01:34:08 JIT=1
01:37:35 what jit does
01:38:50 unittests
01:47:00 bug in tests
01:48:30 don't know anything about computers
01:49:10 tinygrad lines and garbage lines
01:50:00 2k lines, tiyngrad backends
01:51:55 5 lines for speed, shape, reshape
01:53:30 permute, reshape, expr_node
01:54:40 symbolic.py
01:55:20 fixing broken the tests
01:59:22 unittests parametrized test
02:06:00 looking at the docs
02:08:25 optimizers SGD, RMSprop, Adam
02:08:35 test_optim.py, challenge for pull request 2 line
02:09:00 lion optimizer, experimental drugs waiting
02:10:35 admiring mlops.py, looking how long is adam optimizer
02:13:50 llvm=1 on example, slow because copy overhead
02:17:40 tinygrad metal backend faster then pytorch backend
02:18:50 new goals for tinygrad, train imageNet
02:20:40 step to linearize the ast and codegen the ast
02:21:10 what is different about jax
02:29:20 numpy and finding a bug in tinygrad
02:43:15 adding test_div_numerator_negative
02:47:10 thinking about what to do
02:54:50 tinygrad symbolic implementation
03:00:35 removing tests
03:01:55 linking docs to tinygrad discord
03:02:55 commit bufs not none
03:04:45 some abstractions are good
03:05:15 codegen needs new version
03:06:10 goals for 0.6 release
03:07:00 code is a line, ast is a tree 
03:07:30 reviewing code
03:08:55 tinygrad is not for openpilot, is competitor to pytorch
03:11:10 symolic is now a 6/10 due to the infinite loop
03:14:10 read and share the docs with friends
03:14:20 is tinygrad stupid? a lot of time invested into tinygrad
03:14:50 who is tinygrad for?
03:15:30 atan2_cpu
03:16:15 tinygrad stable diffusion
03:17:10 why python
03:18:15 transformers
03:18:33 pyright
03:22:00 tinygrad is just george and community
03:22:33 batman joke in javascript
03:24:10 ENABLE_METHOD_CACHE=0
03:24:40 pytorch is broken
03:26:30 workday start at comma
03:27:35 imagenet to c compile test
03:29:10 tinygrad will win, fast
03:30:00 agi does not exist
03:30:15 gpt-4 this week
03:30:50 ban for bad user
03:31:05 how readable is tinygrad code
03:32:20 plans for tinygrad inference and learning as the same thing
03:32:40 long term re-writing tinygrad in tinygrad
03:33:25 andrej karpathy on training vs inference

Official George Hotz communication channels:
- https://geohot.com
- https://twitter.com/realGeorgeHotz
- https://instagram.com/georgehotz
- https://tinygrad.org
- https://geohot.github.io/blog
- https://twitch.tv/georgehotz
- https://github.com/geohot
- https://youtube.com/geohot

We archive George Hotz and comma.ai videos for fun.
Follow for notifications:  
- https://twitter.com/geohotarchive

Thank you for reading and using the SHOW MORE button.
We hope you enjoy watching George's videos as much as we do.
See you at the next video.