# George Hotz on Entropics and AI Intelligence

George Hotz introduces "entropics" as a proposed field of study that is to intelligence what thermodynamics is to energy. He defines entropics as the science that answers questions like:

- How much intelligence is needed to prove Fermat's Last Theorem?
- How much intelligence is needed to be a 2000 ELO chess player?
- How much intelligence is needed to compress Wikipedia?

## Key concepts discussed:

- **Intelligence measurement**: Hotz proposes the "person" (equivalent to 20 petaflops) as a unit of intelligence, similar to how horsepower measures energy
- **Intelligence as compression**: He suggests intelligence can be viewed as compression capability
- **Computational growth**: He discusses how computational capacity is growing at roughly 25% per year, suggesting this rate is sustainable
- **AI revolution**: He compares the computer/AI revolution to the Industrial Revolution, showing how intelligence will soon no longer be proportional to human population

## AI safety perspectives:

- Hotz expresses skepticism about "recursive self-improvement" and "hard takeoff" scenarios
- He argues that AGI won't experience sudden, explosive growth but will develop more gradually
- He suggests the real danger isn't machines becoming superintelligent overnight, but rather societal issues around AI deployment
- He discusses his upcoming debate with Eliezer Yudkowsky on AI safety

The stream includes tangents on computational efficiency, society's problems, social media, and his company's "tiny box" product (a computational device).